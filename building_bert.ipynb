{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model of Amino-Seq Masked-Language-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "\n",
    "import string\n",
    "from typing import Iterable, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['---LSQF--LLMLWVPGSKGEIVLTQSPASVSVSPGERVTISCQASESVGNTYLNWLQQKSGQSPRWLIYQVSKLESGIPARFRGSGSGTDFTFTISRVEAEDVAHYYSQQ-----',\n",
       " 'MESLSQC--LLMLWVPVSRGAIVLTQSPALVSVSPGERVTISCKASQSVGNTYLSWFRQKPGQSPRGLIYKVSNLPSGVPSRFRGSGAEKDFTLTISRVEAVDGAVYYCAQASYSP',\n",
       " 'MESLSQC--LLMLWVPVSRGAIVLTQSPASVSVSPGERVTISCKASQSLGNTYLHWFQQKPGQSPRRLIYQVSNLLSGVPSRFSGSGAGKDFSLTISSVEAGDGAVYYCFQGSYDP']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'X_set.txt'\n",
    "\n",
    "# Initialize lists to hold the phylogenetic position strings and amino acid sequences\n",
    "specie_code = []\n",
    "amino_acid_sequences = []\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(' ')\n",
    "        specie_code.append(parts[0])\n",
    "        amino_acid_sequences.append(parts[1])\n",
    "\n",
    "amino_acid_sequences[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Tokenizer\n",
    "\n",
    "- There are 20 amino acids, each letter in the chain represents one of them. \n",
    "- Converting them into 20 tokens, meaning each amino acid would get a number associated with it. \n",
    "- Would also need a special character token, which is \"-\", something related to multiple-sequence-alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Amino Acids: 20\n"
     ]
    }
   ],
   "source": [
    "# Creating a set of all amino-acids\n",
    "\n",
    "amino_acid_set = set()\n",
    "\n",
    "for seq in amino_acid_sequences:\n",
    "    for acid in seq:\n",
    "        if acid != \"-\":\n",
    "            amino_acid_set.add(acid)\n",
    "\n",
    "# 20 amino acids\n",
    "print(f\"Num of Amino Acids: {len(amino_acid_set) }\")\n",
    "amino_acids_list = list(amino_acid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Tokenzer class, which ennodes and decodes an amino acid sequence \n",
    "\n",
    "class Tokenizer:\n",
    "    ''' \n",
    "    To encode and decode any amino acid string\n",
    "    '''\n",
    "    # class attribute \n",
    "    amino_acids = amino_acids_list\n",
    "\n",
    "    def __init__(self, special_tokens = Iterable[str]):\n",
    "        # define a vocab\n",
    "        self.vocab = Tokenizer.amino_acids + list(special_tokens)\n",
    "        # mapping each vocab to a token (a numeric value)\n",
    "        self.token2idx = {token:i for i, token in enumerate(self.vocab)} \n",
    "        # mapping numeric value back to a token\n",
    "        self.idx2token = {i:token for token, i  in self.token2idx.items()}\n",
    "\n",
    "    def encode(self, inputs: Iterable[str]) -> Iterable[int]:\n",
    "        return [self.token2idx[token] for token in inputs]\n",
    "    \n",
    "    def decode(self, inputs: Iterable[int]) -> Iterable[str]:\n",
    "        return [self.idx2token[idx] for idx in inputs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 amino acids         : ['-', '-', '-', 'L', 'S', 'Q', 'F', '-', '-', 'L', 'L', 'M', 'L', 'W', 'V', 'P', 'G', 'S', 'K', 'G']\n",
      "First 20 encoded amino acids : [20, 20, 20, 7, 15, 0, 4, 20, 20, 7, 7, 9, 7, 17, 6, 18, 8, 15, 19, 8]\n",
      "First 20 decoded amino acids : ['-', '-', '-', 'L', 'S', 'Q', 'F', '-', '-', 'L', 'L', 'M', 'L', 'W', 'V', 'P', 'G', 'S', 'K', 'G']\n"
     ]
    }
   ],
   "source": [
    "# creating an instance of the Tokenizer. \n",
    "amino_acid_tokenizer = Tokenizer(special_tokens=[\"-\", \"[MASK]\"])\n",
    "\n",
    "# let's encode the first amino-acid-sequence and see the first 10 positions\n",
    "print(f\"First 20 amino acids         : {[i for i in amino_acid_sequences[0][0:20]]}\")\n",
    "print(f\"First 20 encoded amino acids : {amino_acid_tokenizer.encode(amino_acid_sequences[0])[0:20]}\")\n",
    "print(f\"First 20 decoded amino acids : {amino_acid_tokenizer.decode(amino_acid_tokenizer.encode(amino_acid_sequences[0])[0:20])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q': 0, 'I': 1, 'R': 2, 'D': 3, 'F': 4, 'Y': 5, 'V': 6, 'L': 7, 'G': 8, 'M': 9, 'T': 10, 'E': 11, 'C': 12, 'A': 13, 'H': 14, 'S': 15, 'N': 16, 'W': 17, 'P': 18, 'K': 19, '-': 20, '[MASK]': 21}\n"
     ]
    }
   ],
   "source": [
    "print(amino_acid_tokenizer.token2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Creating a Tensor for all amino-seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{116}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making sure that the size of each amino-acid-seq is same\n",
    "\n",
    "len_amino_acid_seq = set()\n",
    "for seq in amino_acid_sequences:\n",
    "    len_amino_acid_seq.add(len(seq))\n",
    "\n",
    "# this set should have only one value \n",
    "len_amino_acid_seq\n",
    "# perfect! all the seq are 116 character long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_amino_acids_tensor(amino_acid_sequences:list, my_tokenizer:Tokenizer):\n",
    "\n",
    "    amino_acid_tensors = []\n",
    "\n",
    "    for seq in amino_acid_sequences:\n",
    "        amino_acid_tensors.append(torch.Tensor(my_tokenizer.encode(seq)).to(torch.int64))\n",
    "\n",
    "    # stacking them \n",
    "    stacked_tensor =  torch.stack(amino_acid_tensors)\n",
    "\n",
    "    return stacked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_amino_acids_tensor = create_amino_acids_tensor(amino_acid_sequences, amino_acid_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1001, 116])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_amino_acids_tensor.shape\n",
    "\n",
    "# 1001 seqs, each with the length of 116"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create Training data \n",
    "\n",
    "- So what we need is to mask a random position in seq\n",
    "- let's only mask one posiiton as of now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MaskedAminoSeqDataset(Dataset):\n",
    "    def __init__(self, input_tensor: torch.Tensor, mask_token: int):\n",
    "            \"\"\"\n",
    "            Dataset for masked amino acid sequence prediction.\n",
    "\n",
    "            Args:\n",
    "            input_tensor (torch.Tensor): Input tensor of shape (num_sequences, sequence_length).\n",
    "            mask_token (int): The token used for masking.\n",
    "            \"\"\"\n",
    "            self.input_tensor = input_tensor\n",
    "            self.mask_token = mask_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_tensor.shape[0] \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seqs, target_amino_acids, mask_positions = \\\n",
    "            self._create_training_data(self.input_tensor, batch_size=1, mask_token=self.mask_token)\n",
    "        return input_seqs.squeeze(0), target_amino_acids.squeeze(0), mask_positions.squeeze(0)\n",
    "\n",
    "    def _create_training_data(self, input_tensor: torch.Tensor, batch_size: int, mask_token: int, min_masks: int = 1, max_masks: int = 5):\n",
    "        \"\"\"\n",
    "        Creates masked training data efficiently using vectorized operations with a random number of masks per sequence.\n",
    "\n",
    "        Args:\n",
    "        input_tensor (torch.Tensor): Input tensor of shape (num_sequences, sequence_length)\n",
    "        batch_size (int): The desired batch size.\n",
    "        mask_token (int): The token used for masking.\n",
    "        min_masks (int): Minimum number of positions to mask in each sequence. Default is 1.\n",
    "        max_masks (int): Maximum number of positions to mask in each sequence. Default is 5.\n",
    "\n",
    "        Returns:\n",
    "        tuple: (input_seqs, target_amino_acids, mask_positions)\n",
    "            - input_seqs: Tensor of shape (batch_size, sequence_length) with masked sequences.\n",
    "            - target_amino_acids: Tensor of shape (batch_size, sequence_length) with scattered target values.\n",
    "            - mask_positions: Tensor of shape (batch_size, max_masks) indicating mask positions.\n",
    "        \"\"\"\n",
    "        rows, seq_len = input_tensor.shape\n",
    "        \n",
    "        # Randomly select 'batch_size' rows (amino acid sequences)\n",
    "        idx = torch.randint(rows, size=(batch_size,))\n",
    "        input_seqs = input_tensor[idx].clone()\n",
    "\n",
    "        # Generate random number of masks for each sequence\n",
    "        num_masks_per_seq = torch.randint(min_masks, max_masks + 1, (batch_size,))\n",
    "\n",
    "        # Create mask_positions tensor\n",
    "        mask_positions = torch.zeros((batch_size, max_masks), dtype=torch.long)\n",
    "        for i, num_masks in enumerate(num_masks_per_seq):\n",
    "            mask_positions[i, :num_masks] = torch.randperm(seq_len)[:num_masks]\n",
    "\n",
    "        # Create target_amino_acids tensor with the same shape as input_seqs\n",
    "        target_amino_acids = torch.zeros_like(input_seqs)\n",
    "        # TODO: This is wrong, as 0 is one of the tokens of Amino Acids\n",
    "\n",
    "        # Create a mask for the selected positions\n",
    "        mask = torch.zeros_like(input_seqs, dtype=torch.bool)\n",
    "\n",
    "        # Use advanced indexing to set the target values and create mask\n",
    "        for i in range(batch_size):\n",
    "            positions = mask_positions[i, :num_masks_per_seq[i]]\n",
    "            target_amino_acids[i, positions] = input_seqs[i, positions]\n",
    "            mask[i, positions] = True\n",
    "\n",
    "        # Apply the mask to replace the target positions with the mask_token\n",
    "        input_seqs[mask] = mask_token\n",
    "\n",
    "        return input_seqs, target_amino_acids, mask_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token id for the MASK\n",
    "amino_acid_tokenizer.encode([\"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming input_tensor is your tensor of amino acid sequences\n",
    "masked_amino_seq_dataset = MaskedAminoSeqDataset(all_amino_acids_tensor, mask_token=21) \n",
    "masked_amino_seq_dataloader = DataLoader(masked_amino_seq_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amino seqs with masked: \n",
      " shape: torch.Size([32, 116]) \n",
      " tensor([[20, 20, 20,  ..., 11,  7, 18],\n",
      "        [ 9,  2,  6,  ..., 15, 10, 18],\n",
      "        [20, 20, 20,  ..., 15, 13, 18],\n",
      "        ...,\n",
      "        [ 9,  2,  6,  ..., 11,  5, 18],\n",
      "        [ 9,  6,  4,  ..., 15, 17, 18],\n",
      "        [20, 20, 20,  ..., 10, 10,  1]])\n",
      "targets amino acid:  \n",
      " shape: torch.Size([32, 116]) \n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "mask posittions:  \n",
      " shape: torch.Size([32, 5]) \n",
      "tensor([[ 89,   3,  31, 106,   0],\n",
      "        [ 40, 111,  71, 100,  38],\n",
      "        [ 25,  26,  39,  87,   0],\n",
      "        [ 56,  40,  85,  64,  93],\n",
      "        [ 52,  89,  50,   0,   0],\n",
      "        [ 80,  78,   0,   0,   0],\n",
      "        [ 54,  94,  37,   0,   0],\n",
      "        [ 77,  29,   0,   0,   0],\n",
      "        [ 15,  70,  17,  61, 100],\n",
      "        [ 65, 106,  82,  41,  62],\n",
      "        [  4,  39,   6,  83,   0],\n",
      "        [ 37,   0,   0,   0,   0],\n",
      "        [110, 101,   0,   0,   0],\n",
      "        [ 91,  52,   0,   0,   0],\n",
      "        [ 75,  52,   0,   0,   0],\n",
      "        [ 10,  21,  34,   0,   0],\n",
      "        [  7,   0,   0,   0,   0],\n",
      "        [ 50,  31,  93,  13,   0],\n",
      "        [107,  57,  45,   0,   0],\n",
      "        [ 50,  15,  25,  17,   0],\n",
      "        [107,   0,   0,   0,   0],\n",
      "        [ 24,   0,   0,   0,   0],\n",
      "        [ 92,  67,   0,   0,   0],\n",
      "        [ 60,   0,   0,   0,   0],\n",
      "        [  9,  86,  13, 106,   0],\n",
      "        [ 28,   0,   0,   0,   0],\n",
      "        [ 81,  47,   0,   0,   0],\n",
      "        [113,  40,  90,   0,   0],\n",
      "        [ 49,   4,  94,  64,   0],\n",
      "        [ 13,  81,   0,   0,   0],\n",
      "        [109,  60,   0,   0,   0],\n",
      "        [ 83,   0,   0,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "## each iteration now gives a batch with 32 data points.\n",
    "i, t, m = 0, 0, 0 \n",
    "for data in masked_amino_seq_dataloader:\n",
    "    print(f\"amino seqs with masked: \\n shape: {data[0].shape} \\n {data[0]}\")\n",
    "    print(f\"targets amino acid:  \\n shape: {data[1].shape} \\n{data[1]}\")\n",
    "    print(f\"mask posittions:  \\n shape: {data[2].shape} \\n{data[2]}\")\n",
    "\n",
    "    i = data[0]\n",
    "    t = data[1]\n",
    "    m = data[2]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embeddings\n",
    "\n",
    "We need to embeddings\n",
    "\n",
    "- amino acid embeddings \n",
    "- position embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, embed_size)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, max_seq_length, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.token = torch.nn.Embedding(vocab_size, embed_size, dtype=torch.float32)\n",
    "        self.position = SinusoidalPositionEncoding(embed_size, max_seq_length=max_seq_length)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        word_embed = self.token(x) \n",
    "        pos_embed = self.position(x)\n",
    "        out = word_embed + pos_embed\n",
    "\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(amino_acid_tokenizer)\n",
    "d_model = 64 # embedding size \n",
    "max_seq_length = masked_amino_seq_dataset.input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb = BERTEmbeddings(vocab_size=vocab_size, embed_size=d_model, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch shape:     torch.Size([32, 116]) \n",
      "embedded batch shape: torch.Size([32, 116, 64])\n"
     ]
    }
   ],
   "source": [
    "## each iteration now gives a batch with 32 data points.\n",
    "for i in masked_amino_seq_dataloader:\n",
    "    print(f\"input batch shape:     {i[0].shape} \")\n",
    "\n",
    "    print(f\"embedded batch shape: {test_emb(i[0]).shape}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi Headed Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.query = torch.nn.Linear(d_model, d_model, dtype=torch.float32)\n",
    "        self.key = torch.nn.Linear(d_model, d_model, dtype=torch.float32)\n",
    "        self.value = torch.nn.Linear(d_model, d_model, dtype=torch.float32)\n",
    "        self.output_linear = torch.nn.Linear(d_model, d_model, dtype=torch.float32)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        query, key, value of shape: (batch_size, max_len, d_model)\n",
    "        mask of shape: (batch_size, 1, 1, max_words)\n",
    "            # Note: mask if not used, it is mainly to tell attention the locations on which \n",
    "                it should not learn much, like padding indexes\n",
    "                - we dont have padding here as of now, so no need it. \n",
    "        \"\"\"\n",
    "        # (batch_size, max_len, d_model)\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value)   \n",
    "        \n",
    "        # (batch_size, max_len, d_model) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        \n",
    "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
    "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n",
    "\n",
    "        # to mask the pads (diff from the other mask) so the attention does not learn from it\n",
    "        # # fill 0 mask with super small number so it wont affect the softmax weight\n",
    "        # # (batch_size, h, max_len, max_len)\n",
    "        # scores = scores.masked_fill(mask == 0, -1e9)    \n",
    "\n",
    "        # (batch_size, h, max_len, max_len)\n",
    "        # softmax to put attention weight for all non-pad tokens\n",
    "        # max_len X max_len matrix of attention\n",
    "        weights = F.softmax(scores, dim=-1)           \n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        context = torch.matmul(weights, value)\n",
    "\n",
    "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, d_model)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "\n",
    "        # (batch_size, max_len, d_model)\n",
    "        return self.output_linear(context)\n",
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, middle_dim=2048, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = torch.nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.activation = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out\n",
    "\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model=768,\n",
    "        heads=12, \n",
    "        feed_forward_hidden=768 * 4, \n",
    "        dropout=0.1\n",
    "        ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadedAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        # embeddings: (batch_size, max_len, d_model)\n",
    "        # encoder mask: (batch_size, 1, 1, max_len)\n",
    "        # result: (batch_size, max_len, d_model)\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        # residual layer\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        # bottleneck\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = MultiHeadedAttention(heads = 16, d_model=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch shape:     torch.Size([32, 116]) \n",
      "mask posiitons shape:   torch.Size([32, 5])\n",
      "embedded batch shape: torch.Size([32, 116, 64])\n",
      "The output from the Attention : torch.Size([32, 116, 64])\n"
     ]
    }
   ],
   "source": [
    "## each iteration now gives a batch with 32 data points.\n",
    "for i in masked_amino_seq_dataloader:\n",
    "    print(f\"input batch shape:     {i[0].shape} \")\n",
    "    print(f\"mask posiitons shape:   {i[2].shape}\")\n",
    "\n",
    "    print(f\"embedded batch shape: {test_emb(i[0]).shape}\")\n",
    "    embded = test_emb(i[0])\n",
    "    mask = i[2]\n",
    "    attention_output = heads(embded, embded, embded,  mask)\n",
    "\n",
    "    print(f\"The output from the Attention : {attention_output.shape}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output from multiheaded attention goes through a little bit of forward passes, because why not!! \n",
    "- so below is a simple Feedforward pass code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, middle_dim=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = torch.nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.activation = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Layer\n",
    "\n",
    "- putting all together, \n",
    "\n",
    "- embedded matrix comes, first it goes to Attention module \n",
    "- Then layer normalization \n",
    "- then a feed forward part \n",
    "- and it again goes through a layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model=768,\n",
    "        heads=12, \n",
    "        feed_forward_hidden=768 * 4, \n",
    "        dropout=0.1\n",
    "        ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(d_model, dtype=torch.float32)\n",
    "        self.self_multihead = MultiHeadedAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model, middle_dim=feed_forward_hidden)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        # embeddings: (batch_size, max_len, d_model)\n",
    "        # encoder mask: (batch_size, 1, 1, max_len)\n",
    "        # result: (batch_size, max_len, d_model)\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        # residual layer\n",
    "        interacted = interacted.to(torch.float32)\n",
    "        embeddings = embeddings.to(torch.float32)\n",
    "\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        # bottleneck\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT model : Bidirectional Encoder Representations from Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model=768, n_layers=12, heads=12, max_seq_length=500, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: vocab_size of total words\n",
    "        :param hidden: BERT model hidden size\n",
    "        :param n_layers: numbers of Transformer blocks(layers)\n",
    "        :param attn_heads: number of attention heads\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.heads = heads\n",
    "\n",
    "        # paper noted they used 4 * hidden_size for ff_network_hidden_size\n",
    "        self.feed_forward_hidden = d_model * 4\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbeddings(vocab_size=vocab_size, embed_size=d_model, max_seq_length=max_seq_length)\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.encoder_blocks = torch.nn.ModuleList(\n",
    "            [EncoderLayer(d_model, heads, d_model * 4, dropout) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # attention masking for padded token\n",
    "\n",
    "        # (batch_size, 1, seq_len, seq_len)\n",
    "        # mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "        # as of now mask has no role to play, it's for not paying attention to Padding idx\n",
    "        mask = torch.Tensor([1])\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # running over multiple transformer blocks\n",
    "        for encoder in self.encoder_blocks:\n",
    "            x = encoder.forward(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(amino_acid_tokenizer)\n",
    "d_model = 64 # embedding size \n",
    "max_seq_length = masked_amino_seq_dataset.input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_encoder_test = BERT(vocab_size=vocab_size, d_model=d_model, n_layers=3, heads=4, max_seq_length=116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch shape:     torch.Size([32, 116]) \n",
      "mask posiitons shape:   torch.Size([32, 5])\n",
      "bert encoder output shape: torch.Size([32, 116, 64])\n"
     ]
    }
   ],
   "source": [
    "# let's see if we can run oe forward pass\n",
    "\n",
    "## each iteration now gives a batch with 32 data points.\n",
    "for i in masked_amino_seq_dataloader:\n",
    "    print(f\"input batch shape:     {i[0].shape} \")\n",
    "    # So far we have not used this .. it to be used at for defining loss\n",
    "    print(f\"mask posiitons shape:   {i[2].shape}\")\n",
    "\n",
    "    # just pass on the raw x data and the bert model does everythign\n",
    "\n",
    "    bert_output = bert_encoder_test(i[0])\n",
    "    print(f\"bert encoder output shape: {bert_output.shape}\")\n",
    "\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the masked amino-acid\n",
    "\n",
    "class MaskedAminoModel(nn.Module):\n",
    "    \"\"\"\n",
    "    predicting origin token from masked input sequence\n",
    "    n-class classification problem, n-class = vocab_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(hidden, vocab_size)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.log_softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTAmino(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Language Model\n",
    "    Masked Language Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert: BERT, vocab_size):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which should be trained\n",
    "        :param vocab_size: total vocab size for masked_lm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.mask_lm = MaskedAminoModel(self.bert.d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bert(x)\n",
    "        return self.mask_lm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_pred_test = BERTAmino(bert = bert_encoder_test, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch shape:     torch.Size([32, 116]) \n",
      "mask posiitons shape:   torch.Size([32, 5])\n",
      "final output shape: torch.Size([32, 116, 22])\n"
     ]
    }
   ],
   "source": [
    "# let's see if we can run oe forward pass\n",
    "fianl_output = 0\n",
    "## each iteration now gives a batch with 32 data points.\n",
    "for i in masked_amino_seq_dataloader:\n",
    "    print(f\"input batch shape:     {i[0].shape} \")\n",
    "    # So far we have not used this .. it to be used at for defining loss\n",
    "    print(f\"mask posiitons shape:   {i[2].shape}\")\n",
    "\n",
    "    # just pass on the raw x data and the bert model does everythig\n",
    "\n",
    "    bert_mask_pred = mask_pred_test(i[0])\n",
    "    print(f\"final output shape: {bert_mask_pred.shape}\")\n",
    "\n",
    "    fianl_output = bert_mask_pred\n",
    "\n",
    "    # wohooo... it predicted the the amino acid vector for each of location\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 116, 22])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fianl_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a Loss Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, target, masks = 0, 0, 0 \n",
    "for data in masked_amino_seq_dataloader:\n",
    "    # print(f\"amino seqs with masked: \\n shape: {data[0].shape} \\n {data[0]}\")\n",
    "    # print(f\"targets amino acid:  \\n shape: {data[1].shape} \\n{data[1]}\")\n",
    "    # print(f\"mask posittions:  \\n shape: {data[2].shape} \\n{data[2]}\")\n",
    "\n",
    "    input = data[0]\n",
    "    target = data[1]\n",
    "    masks = data[2]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm output shapetorch.Size([32, 116, 22])\n",
      "target shape: torch.Size([32, 116])\n"
     ]
    }
   ],
   "source": [
    "mask_lm_output =  mask_pred_test(input)\n",
    "print(f\"lm output shape{mask_lm_output.shape}\")\n",
    "print(f\"target shape: {target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.9999, -3.3467, -2.8532, -2.6441, -3.4476, -4.1010, -3.1388, -3.2681,\n",
       "        -3.9283, -4.5574, -3.1541, -2.8913, -3.9380, -2.1659, -3.0111, -3.1769,\n",
       "        -3.1969, -2.9439, -2.8219, -2.9521, -3.7809, -2.5906],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_lm_output[0,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 22, 116])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_lm_output.permute(0,2,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 22, 116])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# because pytorch requires this in this shape where num of classes comes before the dimentions \n",
    "\n",
    "mask_lm_output.transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2629, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Negative log likelyhood function\n",
    "\n",
    "criterion = nn.NLLLoss(ignore_index=0)\n",
    "criterion(mask_lm_output.transpose(1, 2), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2629, grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the same if used as a funcitonal form\n",
    "F.nll_loss(mask_lm_output.transpose(1, 2), target, ignore_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Traning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTTrainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        train_dataloader, \n",
    "        test_dataloader=None, \n",
    "        lr= 1e-4,\n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.999),\n",
    "        warmup_steps=1000,\n",
    "        log_freq=10000,\n",
    "        device='mps'\n",
    "        ):\n",
    "\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        # Setting the Adam optimizer with hyper-param\n",
    "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        self.optim_schedule = ScheduledOptim(\n",
    "            self.optim, self.model.bert.d_model, n_warmup_steps=warmup_steps\n",
    "            )\n",
    "\n",
    "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
    "        self.criterion = torch.nn.NLLLoss(ignore_index=0)\n",
    "        self.log_freq = log_freq\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "    \n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.test_data, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        \n",
    "        avg_loss = 0.0\n",
    "        \n",
    "        mode = \"train\" if train else \"test\"\n",
    "\n",
    "        # progress bar\n",
    "        data_iter = tqdm.tqdm(\n",
    "            enumerate(data_loader),\n",
    "            desc=\"EP_%s:%d\" % (mode, epoch),\n",
    "            total=len(data_loader),\n",
    "            bar_format=\"{l_bar}{r_bar}\"\n",
    "        )\n",
    "        start_time = time.time()\n",
    "        for _, data in enumerate(data_loader):\n",
    "\n",
    "            # 0. batch_data will be sent into the device(GPU or cpu)\n",
    "            #data = data.to(self.device)\n",
    "            #data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            # 1. forward the next_sentence_prediction and masked_lm model\n",
    "            mask_lm_output = self.model.forward(data[0].to(self.device)) # data at 0 is input\n",
    "\n",
    "            # 2-1. NLL(negative log likelihood) loss of is_next classification result\n",
    "            # next_loss = self.criterion(next_sent_output, data[\"is_next\"])\n",
    "\n",
    "            # 2-2. NLLLoss of predicting masked token word\n",
    "            # transpose to (m, vocab_size, seq_len) vs (m, seq_len)\n",
    "            # criterion(mask_lm_output.view(-1, mask_lm_output.size(-1)), data[\"bert_label\"].view(-1))\n",
    "            mask_loss = self.criterion(mask_lm_output.transpose(1, 2), data[1].to(self.device)) # data at 1 is target\n",
    "\n",
    "            # 2-3. mask_loss : 3.4 Pre-training Procedure\n",
    "            loss =  mask_loss\n",
    "\n",
    "            # 3. backward and optimization only in train\n",
    "            if train:\n",
    "                self.optim_schedule.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim_schedule.step_and_update_lr()\n",
    "\n",
    "            # next sentence prediction accuracy\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"EP{epoch}, {mode}: avg_loss={avg_loss:.6f}, time={elapsed:.2f}s\")\n",
    "\n",
    "            # post_fix = {\n",
    "            #     \"epoch\": epoch,\n",
    "            #     \"iter\": i,\n",
    "            #     \"avg_loss\": avg_loss / (i + 1),\n",
    "            #     \"loss\": loss.item()\n",
    "            # }\n",
    "\n",
    "            # if i % self.log_freq == 0:\n",
    "            #     data_iter.write(str(post_fix))\n",
    "            #     print(\n",
    "            #         f\"EP{epoch}, {mode}: \\\n",
    "            #         avg_loss={avg_loss / len(data_iter)}\" )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# checking if the mps is availabe\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 102550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:   0%|| 0/32 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0, train: avg_loss=98.830059, time=7.36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:   0%|| 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP1, train: avg_loss=92.435315, time=0.84s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:   0%|| 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP2, train: avg_loss=89.371938, time=0.72s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:   0%|| 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP3, train: avg_loss=86.914500, time=0.67s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:   0%|| 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP4, train: avg_loss=81.975970, time=0.69s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:5:   0%|| 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP5, train: avg_loss=77.620508, time=0.73s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:6:   0%|| 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP6, train: avg_loss=75.232580, time=0.69s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:7:   0%|| 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP7, train: avg_loss=74.009016, time=0.65s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:8:   0%|| 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP8, train: avg_loss=72.849724, time=0.72s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:9:   0%|| 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP9, train: avg_loss=70.343302, time=0.70s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:10:   0%|| 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP10, train: avg_loss=69.166628, time=0.67s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:11:   0%|| 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP11, train: avg_loss=67.990653, time=0.67s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:12:   0%|| 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP12, train: avg_loss=66.967089, time=0.64s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:13:   0%|| 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP13, train: avg_loss=66.730579, time=0.66s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:14:   0%|| 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP14, train: avg_loss=64.778711, time=0.70s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:15:   0%|| 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP15, train: avg_loss=63.854363, time=0.75s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:16:   0%|| 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP16, train: avg_loss=63.232361, time=0.76s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:17:   0%|| 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP17, train: avg_loss=62.540929, time=0.67s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:18:   0%|| 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP18, train: avg_loss=60.240434, time=0.64s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:19:   0%|| 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP19, train: avg_loss=58.693107, time=0.68s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training run\n",
    "\n",
    "vocab_size = len(amino_acid_tokenizer)\n",
    "d_model = 64 # embedding size \n",
    "heads = 4\n",
    "n_layers = 2 \n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "max_seq_length = masked_amino_seq_dataset.input_tensor.shape[1]\n",
    "\n",
    "bert_model = BERT(vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, heads=heads, max_seq_length=max_seq_length)\n",
    "\n",
    "bert_lm = BERTAmino(bert_model, vocab_size)\n",
    "bert_trainer = BERTTrainer(bert_lm, masked_amino_seq_dataloader, device=device)\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  bert_trainer.train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (embedding): BERTEmbeddings(\n",
       "    (token): Embedding(22, 64)\n",
       "    (position): SinusoidalPositionEncoding()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder_blocks): ModuleList(\n",
       "    (0-1): 2 x EncoderLayer(\n",
       "      (layernorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_multihead): MultiHeadedAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (output_linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(sequence, mask_token_id, mask_position=None):\n",
    "    # Convert the sequence to a list of tokens\n",
    "    tokens = list(sequence)\n",
    "    \n",
    "    # If mask_position is not provided, choose a random position to mask\n",
    "    if mask_position is None:\n",
    "        mask_position = random.randint(0, len(tokens) - 1)\n",
    "\n",
    "    # Replace the chosen token with the mask token\n",
    "    original_token = tokens[mask_position]\n",
    "    tokens[mask_position] = \"[MASK]\"\n",
    "    \n",
    "    # Convert tokens to ids\n",
    "    input_ids = amino_acid_tokenizer.encode(tokens)\n",
    "    \n",
    "    return torch.tensor([input_ids]), mask_position, original_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "ex_sequence = \"MASLTQC--LLLFWLAGSQGEVVLTQSPASVSVSLGERVTIKCKASQSLGKTYLHWFQQKLGKSIKRTIYQVSNLDSGVPPRFSGSGSGTDFTLTISSLEPEDAAMYYCGQHTHWP\"\n",
    "\n",
    "# Mask a specific position (e.g., the 10th amino acid)\n",
    "input_tensor, mask_position, original_token = prepare_input(ex_sequence,\n",
    "                                                            amino_acid_tokenizer.token2idx[\"[MASK]\"],\n",
    "                                                            mask_position=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_position = 2\n",
      "original token = S\n"
     ]
    }
   ],
   "source": [
    "print(f\"mask_position = {mask_position}\")\n",
    "print(f\"original token = {original_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 116, 22])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = bert_lm(input_tensor.to(device))\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.1034, -3.6875, -4.7891, -2.8770, -4.9880, -6.6007, -2.2603, -4.1596,\n",
       "         -2.5774, -4.4833, -4.4368, -4.6260, -6.9109, -2.1862, -4.9076, -2.4845,\n",
       "         -5.3131, -5.1837, -0.8550, -3.7203, -5.3274, -6.1643]],\n",
       "       device='mps:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the probabilites at the mask_positing \n",
    "\n",
    "# Predicted scores for each of the \n",
    "preds[:,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_masked_token(model, input_tensor, mask_position):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        output = model(input_tensor)\n",
    "        \n",
    "        # Get the prediction for the masked position\n",
    "        masked_token_logits = output[0, mask_position, :]\n",
    "        \n",
    "        # to get back the probabilites from the log-softmax (model's output)\n",
    "        token_probabilities = torch.exp(masked_token_logits)\n",
    "        \n",
    "        return token_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0022, 0.0427, 0.0078, 0.0438, 0.0105, 0.0022, 0.1419, 0.0437, 0.0503,\n",
       "        0.0230, 0.0169, 0.0068, 0.0011, 0.1246, 0.0078, 0.1010, 0.0066, 0.0087,\n",
       "        0.3165, 0.0208, 0.0191, 0.0021], device='mps:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_prob = predict_masked_token(model=bert_lm, input_tensor=input_tensor, mask_position=mask_position)\n",
    "token_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 0.3165\n",
      "V: 0.1419\n",
      "A: 0.1246\n",
      "S: 0.1010\n",
      "G: 0.0503\n",
      "D: 0.0438\n",
      "L: 0.0437\n",
      "I: 0.0427\n",
      "M: 0.0230\n",
      "K: 0.0208\n",
      "-: 0.0191\n",
      "T: 0.0169\n",
      "F: 0.0105\n",
      "W: 0.0087\n",
      "R: 0.0078\n",
      "H: 0.0078\n",
      "E: 0.0068\n",
      "N: 0.0066\n",
      "Y: 0.0022\n",
      "Q: 0.0022\n",
      "[MASK]: 0.0021\n",
      "C: 0.0011\n"
     ]
    }
   ],
   "source": [
    "def interpret_predictions(token_probabilities, tokenizer):\n",
    "    sorted_probs, sorted_indices = torch.sort(token_probabilities, descending=True)\n",
    "    for prob, index in zip(sorted_probs, sorted_indices):\n",
    "        token = tokenizer.idx2token[index.item()]\n",
    "        print(f\"{token}: {prob.item():.4f}\")\n",
    "\n",
    "# Example usage\n",
    "interpret_predictions(token_prob, amino_acid_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' amino_bert/\\n    data/\\n       __init__.py\\n       dataset.py\\n       tokenizer.py\\n    models/\\n       __init__.py\\n       bert.py\\n       embeddings.py\\n       attention.py\\n    training/\\n       __init__.py\\n       trainer.py\\n       optimizer.py\\n    utils/\\n       __init__.py\\n       helpers.py\\n    config.py\\n    main.py\\n    requirements.txt\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Productionalization of this whole code. \n",
    "\n",
    "\"\"\" amino_bert/\n",
    "    data/\n",
    "       __init__.py\n",
    "       dataset.py\n",
    "       tokenizer.py\n",
    "    models/\n",
    "       __init__.py\n",
    "       bert.py\n",
    "       embeddings.py\n",
    "       attention.py\n",
    "    training/\n",
    "       __init__.py\n",
    "       trainer.py\n",
    "       optimizer.py\n",
    "    utils/\n",
    "       __init__.py\n",
    "       helpers.py\n",
    "    config.py\n",
    "    main.py\n",
    "    requirements.txt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
