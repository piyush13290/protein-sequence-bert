{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model for understanding mutation in protien sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "\n",
    "import string\n",
    "from typing import Iterable, Tuple, Optional, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['---LSQF--LLMLWVPGSKGEIVLTQSPASVSVSPGERVTISCQASESVGNTYLNWLQQKSGQSPRWLIYQVSKLESGIPARFRGSGSGTDFTFTISRVEAEDVAHYYSQQ-----',\n",
       " 'MESLSQC--LLMLWVPVSRGAIVLTQSPALVSVSPGERVTISCKASQSVGNTYLSWFRQKPGQSPRGLIYKVSNLPSGVPSRFRGSGAEKDFTLTISRVEAVDGAVYYCAQASYSP',\n",
       " 'MESLSQC--LLMLWVPVSRGAIVLTQSPASVSVSPGERVTISCKASQSLGNTYLHWFQQKPGQSPRRLIYQVSNLLSGVPSRFSGSGAGKDFSLTISSVEAGDGAVYYCFQGSYDP']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'X_set.txt'\n",
    "\n",
    "# Initialize lists to hold the phylogenetic position strings and amino acid sequences\n",
    "specie_code = []\n",
    "amino_acid_sequences = []\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(' ')\n",
    "        specie_code.append(parts[0])\n",
    "        amino_acid_sequences.append(parts[1])\n",
    "\n",
    "amino_acid_sequences[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amino_acid_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we would need two more set of information \n",
    "\n",
    "# 1. protien type \n",
    "protein_types = ['A1'] * len(amino_acid_sequences)\n",
    "\n",
    "# 2. weights for species\n",
    "specie_weight = torch.rand(len(amino_acid_sequences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Tokenizer\n",
    "\n",
    "- There are 20 amino acids, each letter in the chain represents one of them. \n",
    "- Converting them into 20 tokens, meaning each amino acid would get a number associated with it. \n",
    "- Would also need a special character token, which is \"-\", something related to multiple-sequence-alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Amino Acids: 20\n"
     ]
    }
   ],
   "source": [
    "# making sure that there are only 20 diff types of amino acids \n",
    "\n",
    "amino_acid_set = set()\n",
    "\n",
    "for seq in amino_acid_sequences:\n",
    "    for acid in seq:\n",
    "        if acid != \"-\":\n",
    "            amino_acid_set.add(acid)\n",
    "\n",
    "# 20 amino acids\n",
    "print(f\"Num of Amino Acids: {len(amino_acid_set) }\")\n",
    "amino_acids_list = list(amino_acid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Tokenzer class, which ennodes and decodes an amino acid sequence \n",
    "\n",
    "class AminoAcidTokenizer:\n",
    "    ''' \n",
    "    To encode and decode any amino acid string\n",
    "    '''\n",
    "    # class attribute\n",
    "    # all 20 types of amino acids\n",
    "    amino_acids = ['S','D','H','L','T','E','W','N','Y','Q','C','G','V','K','I','R','M','F','A','P']\n",
    "\n",
    "    def __init__(self, special_tokens: Optional[Iterable[str]] = None):\n",
    "        # define a vocab\n",
    "        self.vocab = AminoAcidTokenizer.amino_acids\n",
    "        if special_tokens:\n",
    "            self.vocab += list(special_tokens)\n",
    "        # mapping each vocab to a token (a numeric value)\n",
    "        self.token2idx = {token:i for i, token in enumerate(self.vocab)} \n",
    "        # mapping numeric value back to a token\n",
    "        self.idx2token = {i:token for token, i  in self.token2idx.items()}\n",
    "\n",
    "    def encode(self, inputs: Iterable[str]) -> Iterable[int]:\n",
    "        return [self.token2idx[token] for token in inputs]\n",
    "    \n",
    "    def decode(self, inputs: Iterable[int]) -> Iterable[str]:\n",
    "        return [self.idx2token[idx] for idx in inputs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 amino acids         : ['-', '-', '-', 'L', 'S', 'Q', 'F', '-', '-', 'L', 'L', 'M', 'L', 'W', 'V', 'P', 'G', 'S', 'K', 'G']\n",
      "First 20 encoded amino acids : [20, 20, 20, 3, 0, 9, 17, 20, 20, 3, 3, 16, 3, 6, 12, 19, 11, 0, 13, 11]\n",
      "First 20 decoded amino acids : ['-', '-', '-', 'L', 'S', 'Q', 'F', '-', '-', 'L', 'L', 'M', 'L', 'W', 'V', 'P', 'G', 'S', 'K', 'G']\n"
     ]
    }
   ],
   "source": [
    "# creating an instance of the Tokenizer. \n",
    "amino_acid_tokenizer = AminoAcidTokenizer(special_tokens=[\"-\", \"[MASK]\", \"[PAD]\"])\n",
    "\n",
    "# let's encode the first amino-acid-sequence and see the first 10 positions\n",
    "print(f\"First 20 amino acids         : {[i for i in amino_acid_sequences[0][0:20]]}\")\n",
    "print(f\"First 20 encoded amino acids : {amino_acid_tokenizer.encode(amino_acid_sequences[0])[0:20]}\")\n",
    "print(f\"First 20 decoded amino acids : {amino_acid_tokenizer.decode(amino_acid_tokenizer.encode(amino_acid_sequences[0])[0:20])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'S': 0, 'D': 1, 'H': 2, 'L': 3, 'T': 4, 'E': 5, 'W': 6, 'N': 7, 'Y': 8, 'Q': 9, 'C': 10, 'G': 11, 'V': 12, 'K': 13, 'I': 14, 'R': 15, 'M': 16, 'F': 17, 'A': 18, 'P': 19, '-': 20, '[MASK]': 21, '[PAD]': 22}\n"
     ]
    }
   ],
   "source": [
    "# all tokens mapped to an idx\n",
    "print(amino_acid_tokenizer.token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amino_acid_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to Amino Acid tokenizers, creating Protien tokenizer\n",
    "\n",
    "class ProteinTokenizer:\n",
    "    '''\n",
    "    To encode and decode protein types and amino acid sequences\n",
    "    '''\n",
    "    # class attribute\n",
    "    protiens = ['A1', 'A2']\n",
    "\n",
    "    def __init__(self, special_tokens: Iterable[str] = None):\n",
    "        # define a vocab\n",
    "        self.vocab = ProteinTokenizer.protiens \n",
    "        if special_tokens:   \n",
    "            self.vocab += list(special_tokens)\n",
    "        # mapping each vocab to a token (a numeric value)\n",
    "        self.token2idx = {token:i for i, token in enumerate(self.vocab)} \n",
    "        # mapping numeric value back to a token\n",
    "        self.idx2token = {i:token for token, i  in self.token2idx.items()}\n",
    "\n",
    "    def encode(self, inputs: Iterable[str]) -> Iterable[int]:\n",
    "        return [self.token2idx[token] for token in inputs]\n",
    "    \n",
    "    def decode(self, inputs: Iterable[int]) -> Iterable[str]:\n",
    "        return [self.idx2token[idx] for idx in inputs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A1': 0, 'A2': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_tokenizer = ProteinTokenizer()\n",
    "protein_tokenizer.token2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing all protein seq and protein types \n",
    "def create_encoded_tensors(\n",
    "    amino_acid_sequences: List[str],\n",
    "    protein_types: List[str],\n",
    "    amino_acid_tokenizer: AminoAcidTokenizer,\n",
    "    protein_tokenizer: ProteinTokenizer\n",
    ") -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Create encoded tensors for amino acid sequences and protein types.\n",
    "\n",
    "    This function takes lists of amino acid sequences and protein types,\n",
    "    and encodes them using the provided tokenizers.\n",
    "\n",
    "    Args:\n",
    "        amino_acid_sequences (List[str]): List of amino acid sequences.\n",
    "        protein_types (List[str]): List of protein types.\n",
    "        amino_acid_tokenizer (AminoAcidTokenizer): Tokenizer for amino acid sequences.\n",
    "        protein_tokenizer (ProteinTokenizer): Tokenizer for protein types.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[torch.Tensor], List[torch.Tensor]]: A tuple containing two lists:\n",
    "            1. List of encoded amino acid sequences as tensors.\n",
    "            2. List of encoded protein types as tensors.\n",
    "    \"\"\"\n",
    "    amino_acid_tensors = []\n",
    "    protein_type_tensors = []\n",
    "\n",
    "    for seq, p_type in zip(amino_acid_sequences, protein_types):\n",
    "        amino_acid_tensors.append(torch.tensor(amino_acid_tokenizer.encode(seq), dtype=torch.int64))\n",
    "        protein_type_tensors.append(torch.tensor(protein_tokenizer.encode([p_type]), dtype=torch.int64))\n",
    "\n",
    "    return amino_acid_tensors, protein_type_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_amino_acids, encoded_protein_types = create_encoded_tensors(\n",
    "    amino_acid_sequences, \n",
    "    protein_types, \n",
    "    amino_acid_tokenizer, \n",
    "    protein_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n",
      "1001\n"
     ]
    }
   ],
   "source": [
    "# both list to have same len\n",
    "print(len(encoded_amino_acids))\n",
    "print(len(encoded_protein_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MaskedAminoSeqDataset(Dataset):\n",
    "    def __init__(self, encoded_amino_acids: list,\n",
    "                encoded_protein_types: list,\n",
    "                specie_weight : torch.Tensor,\n",
    "                mask_token: int,\n",
    "                pad_token: int,\n",
    "                max_len: int ):\n",
    "            \"\"\"\n",
    "            Dataset for masked amino acid sequence prediction.\n",
    "\n",
    "            Args:\n",
    "                encoded_amino_acids (list): List of encoded amino acid sequences.\n",
    "                encoded_protein_types (list): List of encoded protein types.\n",
    "                specie_weight (torch.Tensor): Weight associated with each species.\n",
    "                mask_token (int): The token used for masking.\n",
    "                pad_token (int): The token used for padding.\n",
    "                max_len (int): Maximum length of the sequences.\n",
    "            \"\"\"\n",
    "            self.encoded_amino_acids = encoded_amino_acids\n",
    "            self.encoded_protein_types = encoded_protein_types\n",
    "            self.specie_weight = specie_weight\n",
    "            self.mask_token = mask_token\n",
    "            self.pad_token = pad_token\n",
    "            self.max_len = max_len\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_amino_acids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seqs, target_amino_acids, mask_positions, encoded_protien, sample_weight, padding_masks = \\\n",
    "            self._create_training_data(self.encoded_amino_acids[idx],\n",
    "                                        self.encoded_protein_types[idx],\n",
    "                                        self.specie_weight[idx],\n",
    "                                        self.mask_token,\n",
    "                                        self.pad_token,\n",
    "                                        self.max_len, \n",
    "                                        )\n",
    "        \n",
    "        return input_seqs.squeeze(0), target_amino_acids.squeeze(0), mask_positions.squeeze(0), encoded_protien, sample_weight, padding_masks\n",
    "\n",
    "\n",
    "    def _create_training_data(self, encoded_amino_acid: torch.Tensor,\n",
    "                            encoded_protein_type: torch.Tensor, \n",
    "                            specie_weight: torch.Tensor, \n",
    "                            mask_token: int,\n",
    "                            pad_token: int,\n",
    "                            max_len: int,  \n",
    "                            min_masks: int = 1,\n",
    "                            max_masks: int = 5):\n",
    "        \"\"\"\n",
    "        Create training data for masked amino acid sequence prediction.\n",
    "\n",
    "        This function takes an encoded amino acid sequence and applies random masking\n",
    "        to create input-target pairs for training a BERT-like model. It also handles\n",
    "        padding or truncation to ensure consistent sequence length.\n",
    "\n",
    "        Args:\n",
    "            encoded_amino_acid (torch.Tensor): Encoded amino acid sequence.\n",
    "            encoded_protein_type (torch.Tensor): Encoded protein type.\n",
    "            specie_weight (torch.Tensor): Weight associated with the species.\n",
    "            mask_token (int): Token used for masking.\n",
    "            pad_token (int): Token used for padding.\n",
    "            max_len (int): Maximum length of the sequence.\n",
    "            min_masks (int, optional): Minimum number of tokens to mask. Defaults to 1.\n",
    "            max_masks (int, optional): Maximum number of tokens to mask. Defaults to 5.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - masked_seq (torch.Tensor): Input sequence with masked tokens.\n",
    "                - target_seq (torch.Tensor): Target sequence for masked token prediction.\n",
    "                - fixed_mask_positions (torch.Tensor): Fixed-size tensor of mask positions.\n",
    "                - encoded_protein_type (torch.Tensor): Encoded protein type.\n",
    "                - specie_weight (torch.Tensor): Weight associated with the species.\n",
    "\n",
    "        Notes:\n",
    "            - The function pads or truncates the input sequence to `max_len`.\n",
    "            - It randomly masks between `min_masks` and `max_masks` tokens.\n",
    "            - The `fixed_mask_positions` tensor has a fixed size of `max_masks`,\n",
    "            with -1 values indicating unused mask positions.\n",
    "            - Target sequences use -100 for non-masked positions (ignored in loss calculation).\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the original sequence length\n",
    "        seq_len = encoded_amino_acid.shape[0]\n",
    "\n",
    "        # Determine number of masks (based on the original sequence length)\n",
    "        num_masks = torch.randint(min_masks, min(max_masks + 1, seq_len + 1), (1,)).item()\n",
    "\n",
    "        # Create mask positions (based on the actual sequence length)\n",
    "        mask_positions = torch.randperm(seq_len)[:num_masks]\n",
    "\n",
    "        # Create masked input sequence\n",
    "        masked_seq = encoded_amino_acid.clone()\n",
    "        masked_seq[mask_positions] = mask_token\n",
    "\n",
    "        # Pad or truncate the masked sequence to max_len\n",
    "        if seq_len < max_len:\n",
    "            padding = torch.full((max_len - seq_len,), pad_token, dtype=encoded_amino_acid.dtype)\n",
    "            input_seq = torch.cat([masked_seq, padding])\n",
    "            # Adjust mask_positions for padding\n",
    "            mask_positions = torch.cat([mask_positions, torch.full((max_masks - num_masks,), -1, dtype=torch.long)])\n",
    "        else:\n",
    "            input_seq = masked_seq[:max_len]\n",
    "            # Adjust mask_positions for truncation\n",
    "            mask_positions = mask_positions[mask_positions < max_len]\n",
    "            mask_positions = torch.cat([mask_positions, torch.full((max_masks - mask_positions.shape[0],), -1, dtype=torch.long)])\n",
    "\n",
    "        # Create target sequence\n",
    "        target_seq = torch.full((max_len,), -100, dtype=input_seq.dtype)\n",
    "        target_seq[:seq_len][mask_positions[mask_positions != -1]] = encoded_amino_acid[mask_positions[mask_positions != -1]]\n",
    "    \n",
    "        # a vector to mark if the padding locations\n",
    "        # 1 = real useful data and 0 = padded token (dont learn from it)\n",
    "        padding_masks = torch.arange(max_len) < min(seq_len, max_len)\n",
    "\n",
    "        #print(padding_mask.shape)\n",
    "        # Ensure encoded_protein_type is the right shape\n",
    "        if encoded_protein_type.dim() == 0:\n",
    "            encoded_protein_type = encoded_protein_type.unsqueeze(0)\n",
    "\n",
    "\n",
    "        return input_seq.squeeze(0), target_seq, mask_positions.squeeze(0), encoded_protein_type, specie_weight, padding_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming input_tensor is your tensor of amino acid sequences\n",
    "masked_amino_seq_dataset = MaskedAminoSeqDataset(\n",
    "    encoded_amino_acids=encoded_amino_acids, \n",
    "    encoded_protein_types=encoded_protein_types, \n",
    "    specie_weight=specie_weight,\n",
    "    mask_token=21,\n",
    "    pad_token=22, \n",
    "    max_len=120\n",
    ") \n",
    "masked_amino_seq_dataloader = DataLoader(masked_amino_seq_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amino seqs with masked: shape: torch.Size([32, 120])\n",
      "targets amino acid:     shape: torch.Size([32, 120])\n",
      "mask posittions:        shape: torch.Size([32, 5]) \n",
      "encoded protein type:   shpae: torch.Size([32, 1])\n",
      "specie_weight:          shape: torch.Size([32])\n",
      "padding mask indicator  shape: torch.Size([32, 120])\n"
     ]
    }
   ],
   "source": [
    "## each iteration now gives a batch with 32 data points.\n",
    "amino_seqs, targets, mask_pos, protien, weight, padding_masks = 0, 0, 0, 0, 0, 0\n",
    "for data in masked_amino_seq_dataloader:\n",
    "    print(f\"amino seqs with masked: shape: {data[0].shape}\")\n",
    "    print(f\"targets amino acid:     shape: {data[1].shape}\")\n",
    "    print(f\"mask posittions:        shape: {data[2].shape} \")\n",
    "    print(f\"encoded protein type:   shpae: {data[3].shape}\")\n",
    "    print(f\"specie_weight:          shape: {data[4].shape}\")\n",
    "    print(f\"padding mask indicator  shape: {data[5].shape}\")\n",
    "\n",
    "    amino_seqs = data[0]\n",
    "    targets = data[1]\n",
    "    mask_pos = data[2]\n",
    "    protien = data[3]\n",
    "    weight = data[4]\n",
    "    paddx = data[5]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single amino acid seq: tensor([16,  5,  3, 14,  0,  9, 17, 12, 17, 20,  3,  3,  3,  6,  3,  0, 11, 18,\n",
      "         1, 11, 21, 14, 12, 16,  4,  9,  0, 21, 13,  0, 16,  0, 14,  0, 12, 11,\n",
      "         1, 15, 12,  4, 16,  7, 10, 13, 18,  0,  9,  7, 12, 20,  8,  7,  7, 14,\n",
      "        18, 21,  8,  9,  9, 13, 19, 11,  9,  0, 19, 13,  3,  3, 14,  8,  8, 18,\n",
      "         0,  7, 15,  8,  7, 11, 12, 19,  1, 15, 17,  4, 11,  0, 11,  8, 11,  4,\n",
      "         1, 17,  4,  3,  4, 14,  7,  0, 12, 21, 18,  5,  1, 18, 18, 17,  8,  8,\n",
      "        10,  9, 15, 14,  8,  7, 21, 19, 22, 22, 22, 22])\n",
      "target amino acid: tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100,    7, -100, -100, -100,\n",
      "        -100, -100, -100,   19, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100,    6, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100,    9, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100,    0, -100, -100, -100, -100, -100])\n",
      "mask position: tensor([114,  27,  20,  99,  55])\n",
      "protien : tensor([0])\n",
      "weight : 0.13813281059265137\n",
      "padding masks : tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True, False, False, False, False])\n"
     ]
    }
   ],
   "source": [
    "# first amino-acid-seq\n",
    "print(f\"single amino acid seq: {amino_seqs[0]}\")\n",
    "print(f\"target amino acid: {targets[0]}\")\n",
    "print(f\"mask position: {mask_pos[0]}\")\n",
    "print(f\"protien : {protien[0]}\")\n",
    "print(f\"weight : {weight[0]}\")\n",
    "print(f\"padding masks : {paddx[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- amino acid embeddings \n",
    "- position embeddings \n",
    "- protein type embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal Positional Encoding module.\n",
    "\n",
    "    This module generates sinusoidal position embeddings for input sequences.\n",
    "    It can create up to `max_seq_length` unique position embeddings (default 5000).\n",
    "\n",
    "    Args:\n",
    "        embed_size (int): The size of each embedding vector.\n",
    "        max_seq_length (int, optional): The maximum sequence length to support. \n",
    "            Defaults to 5000.\n",
    "\n",
    "    Attributes:\n",
    "        embed_size (int): The size of each embedding vector.\n",
    "        pe (Tensor): The pre-computed position encoding matrix of shape \n",
    "            (1, max_seq_length, embed_size).\n",
    "\n",
    "    Note:\n",
    "        - The actual number of unique embeddings used depends on the input \n",
    "          sequence length in the forward pass.\n",
    "        - While there are `max_seq_length` distinct vectors, positions beyond \n",
    "          this could theoretically be represented due to the periodic nature \n",
    "          of sine and cosine functions, albeit with some loss of uniqueness.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, embed_size)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, amino_vocab_size, protien_vocab_size, embed_size, max_seq_length, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.amino_acid_token = torch.nn.Embedding(amino_vocab_size, embed_size, dtype=torch.float32)\n",
    "        self.position = SinusoidalPositionEncoding(embed_size, max_seq_length=max_seq_length)\n",
    "        self.protien_token = torch.nn.Embedding(protien_vocab_size, embed_size, dtype=torch.float32)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, amino_acid_seqs, protiens):\n",
    "        \"\"\"\n",
    "        amino_acid_seqs = (B * C) ; protien =  (32 * 1)\n",
    "        output ===> (B * C * d_model)\n",
    "        \"\"\"\n",
    "    \n",
    "        amino_acid_embed = self.amino_acid_token(amino_acid_seqs) \n",
    "        pos_embed = self.position(amino_acid_seqs)\n",
    "        protien_embed = self.protien_token(protiens)\n",
    "        # to see the dim of each embeddings\n",
    "        # print(amino_acid_embed.shape)\n",
    "        # print(pos_embed.shape)\n",
    "        # print(protien_embed.shape)\n",
    "        out = amino_acid_embed + pos_embed + protien_embed\n",
    "\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example \n",
    "amino_vocab_size = len(amino_acid_tokenizer)\n",
    "protein_vocab_size = len(protein_tokenizer)\n",
    "d_model = 64 # embedding size \n",
    "max_seq_length = 200 # this doen't have to be precise, this is only for positional encoding\n",
    "\n",
    "\n",
    "test_emb = BERTEmbeddings(amino_vocab_size=amino_vocab_size,\n",
    "                        protien_vocab_size=protein_vocab_size,\n",
    "                        embed_size=d_model,\n",
    "                        max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch shape:     torch.Size([32, 120]) \n",
      "embedded batch shape: torch.Size([32, 120, 64])\n"
     ]
    }
   ],
   "source": [
    "## each iteration now gives a batch with 32 data points.\n",
    "for data in masked_amino_seq_dataloader:\n",
    "    print(f\"input batch shape:     {data[0].shape} \")\n",
    "\n",
    "    print(f\"embedded batch shape: {test_emb(data[0], data[3]).shape}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi Headed Attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.query = torch.nn.Linear(d_model, d_model, dtype=torch.float32)\n",
    "        self.key = torch.nn.Linear(d_model, d_model, dtype=torch.float32)\n",
    "        self.value = torch.nn.Linear(d_model, d_model, dtype=torch.float32)\n",
    "        self.output_linear = torch.nn.Linear(d_model, d_model, dtype=torch.float32)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        query, key, value of shape: (batch_size, max_len, d_model)\n",
    "        mask of shape: (batch_size, 1, 1, max_words)\n",
    "            # Note: mask is not used, it is mainly to tell attention the locations on which \n",
    "                it should not learn much, like padding indexes\n",
    "                - we dont have padding here as of now, so no need it. \n",
    "                # TODO: add the use of padding tokens. \n",
    "        \"\"\"\n",
    "        # (batch_size, max_len, d_model)\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value)   \n",
    "        \n",
    "        # (batch_size, max_len, d_model) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        \n",
    "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
    "        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n",
    "\n",
    "        # to mask the pads (diff from the other mask) so the attention does not learn from it\n",
    "        # # fill 0 mask with super small number so it wont affect the softmax weight\n",
    "        # # (batch_size, h, max_len, max_len)\n",
    "        scores = scores.masked_fill(mask.view(32, 1, 1, 120) == 0, -1e9)    \n",
    "\n",
    "        # (batch_size, h, max_len, max_len)\n",
    "        # softmax to put attention weight for all non-pad tokens\n",
    "        # max_len X max_len matrix of attention\n",
    "        weights = F.softmax(scores, dim=-1)           \n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        context = torch.matmul(weights, value)\n",
    "\n",
    "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, d_model)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "\n",
    "        # (batch_size, max_len, d_model)\n",
    "        return self.output_linear(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = MultiHeadedAttention(heads = 16, d_model=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch shape:     torch.Size([32, 120]) \n",
      "mask posiitons shape:   torch.Size([32, 5])\n",
      "embedded batch shape: torch.Size([32, 120, 64])\n",
      "The output from the Attention : torch.Size([32, 120, 64])\n"
     ]
    }
   ],
   "source": [
    "## each iteration now gives a batch with 32 data points.\n",
    "for i in masked_amino_seq_dataloader:\n",
    "    print(f\"input batch shape:     {i[0].shape} \")\n",
    "    print(f\"mask posiitons shape:   {i[2].shape}\")\n",
    "    embded = test_emb(data[0], data[3])\n",
    "    print(f\"embedded batch shape: {embded.shape}\")\n",
    "    mask = i[5]\n",
    "    attention_output = heads(embded, embded, embded,  mask)\n",
    "\n",
    "    print(f\"The output from the Attention : {attention_output.shape}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
