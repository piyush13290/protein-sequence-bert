{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Amino Acid Seq Data --> creating input Tensor\n",
    "\n",
    "1. Loading the data \n",
    "2. Create a Tokenizer for amino acids\n",
    "3. Create a Tensor object \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. notebook init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "\n",
    "import string\n",
    "from typing import Iterable, Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "\n",
    "- The data is in .txt file, somewhat in a format for two columns. the first column is species-code and the next one is the amino-acid-seq  \n",
    "- The simplest way to get the data is create lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'X_set.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to hold the phylogenetic position strings and amino acid sequences\n",
    "specie_code = []\n",
    "amino_acid_sequences = []\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(' ')\n",
    "        specie_code.append(parts[0])\n",
    "        amino_acid_sequences.append(parts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['111133333333333333333333333333',\n",
       " '111211333333333333333333333333',\n",
       " '111212333333333333333333333333']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specie_code[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['---LSQF--LLMLWVPGSKGEIVLTQSPASVSVSPGERVTISCQASESVGNTYLNWLQQKSGQSPRWLIYQVSKLESGIPARFRGSGSGTDFTFTISRVEAEDVAHYYSQQ-----',\n",
       " 'MESLSQC--LLMLWVPVSRGAIVLTQSPALVSVSPGERVTISCKASQSVGNTYLSWFRQKPGQSPRGLIYKVSNLPSGVPSRFRGSGAEKDFTLTISRVEAVDGAVYYCAQASYSP',\n",
       " 'MESLSQC--LLMLWVPVSRGAIVLTQSPASVSVSPGERVTISCKASQSLGNTYLHWFQQKPGQSPRRLIYQVSNLLSGVPSRFSGSGAGKDFSLTISSVEAGDGAVYYCFQGSYDP']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amino_acid_sequences[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a Tokenizer for amino acids\n",
    "\n",
    "- There are 20 amino acids, each letter in the chain represents one of them. \n",
    "- Converting them into 20 tokens, meaning each amino acid would get a number associated with it. \n",
    "- Would also need a special character token, which is \"-\", something related to multiple-sequence-alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Amino Acids: 20\n"
     ]
    }
   ],
   "source": [
    "# Creating a set of all amino-acids\n",
    "\n",
    "amino_acid_set = set()\n",
    "\n",
    "for seq in amino_acid_sequences:\n",
    "    for acid in seq:\n",
    "        if acid != \"-\":\n",
    "            amino_acid_set.add(acid)\n",
    "\n",
    "# 20 amino acids\n",
    "print(f\"Num of Amino Acids: {len(amino_acid_set) }\")\n",
    "amino_acids_list = list(amino_acid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Tokenzer class, which ennodes and decodes an amino acid sequence \n",
    "\n",
    "class Tokenizer:\n",
    "    ''' \n",
    "    To encode and decode any amino acid string\n",
    "    '''\n",
    "    # class attribute \n",
    "    amino_acids = amino_acids_list\n",
    "\n",
    "    def __init__(self, special_tokens = Iterable[str]):\n",
    "        # define a vocab\n",
    "        self.vocab = Tokenizer.amino_acids + list(special_tokens)\n",
    "        # mapping each vocab to a token (a numeric value)\n",
    "        self.token2idx = {token:i for i, token in enumerate(self.vocab)} \n",
    "        # mapping numeric value back to a token\n",
    "        self.idx2token = {i:token for token, i  in self.token2idx.items()}\n",
    "\n",
    "    def encode(self, inputs: Iterable[str]) -> Iterable[int]:\n",
    "        return [self.token2idx[token] for token in inputs]\n",
    "    \n",
    "    def decode(self, inputs: Iterable[int]) -> Iterable[str]:\n",
    "        return [self.idx2token[idx] for idx in inputs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an instance of the Tokenizer. \n",
    "amino_acid_tokenizer = Tokenizer(special_tokens=[\"-\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 amino acids         : ['-', '-', '-', 'L', 'S', 'Q', 'F', '-', '-', 'L', 'L', 'M', 'L', 'W', 'V', 'P', 'G', 'S', 'K', 'G']\n",
      "First 20 encoded amino acids : [20, 20, 20, 19, 3, 12, 2, 20, 20, 19, 19, 4, 19, 1, 9, 16, 10, 3, 13, 10]\n",
      "First 20 decoded amino acids : ['-', '-', '-', 'L', 'S', 'Q', 'F', '-', '-', 'L', 'L', 'M', 'L', 'W', 'V', 'P', 'G', 'S', 'K', 'G']\n"
     ]
    }
   ],
   "source": [
    "# let's encode the first amino-acid-sequence and see the first 10 positions\n",
    "print(f\"First 20 amino acids         : {[i for i in amino_acid_sequences[0][0:20]]}\")\n",
    "print(f\"First 20 encoded amino acids : {amino_acid_tokenizer.encode(amino_acid_sequences[0])[0:20]}\")\n",
    "print(f\"First 20 decoded amino acids : {amino_acid_tokenizer.decode(amino_acid_tokenizer.encode(amino_acid_sequences[0])[0:20])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amino_acid_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'W': 1, 'F': 2, 'S': 3, 'M': 4, 'C': 5, 'Y': 6, 'I': 7, 'D': 8, 'V': 9, 'G': 10, 'H': 11, 'Q': 12, 'K': 13, 'R': 14, 'N': 15, 'P': 16, 'T': 17, 'E': 18, 'L': 19, '-': 20, '[MASK]': 21}\n"
     ]
    }
   ],
   "source": [
    "print(amino_acid_tokenizer.token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 21]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amino_acid_tokenizer.encode([\"A\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating a Tensor object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{116}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making sure that the size of each amino-acid-seq is same\n",
    "\n",
    "len_amino_acid_seq = set()\n",
    "for seq in amino_acid_sequences:\n",
    "    len_amino_acid_seq.add(len(seq))\n",
    "\n",
    "# this set should have only one value \n",
    "len_amino_acid_seq\n",
    "# perfect! all the seq are 116 character long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_amino_acids_tensor(amino_acid_sequences:list, my_tokenizer:Tokenizer):\n",
    "\n",
    "    amino_acid_tensors = []\n",
    "\n",
    "    for seq in amino_acid_sequences:\n",
    "        amino_acid_tensors.append(torch.Tensor(my_tokenizer.encode(seq)).to(torch.int64))\n",
    "\n",
    "    # stacking them \n",
    "    stacked_tensor =  torch.stack(amino_acid_tensors)\n",
    "\n",
    "    return stacked_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_amino_acids_tensor = create_amino_acids_tensor(amino_acid_sequences, amino_acid_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20, 20, 20,  ..., 20, 20, 20],\n",
       "        [ 4, 18,  3,  ...,  6,  3, 16],\n",
       "        [ 4, 18,  3,  ...,  6,  8, 16],\n",
       "        ...,\n",
       "        [20, 20, 20,  ..., 18,  8, 16],\n",
       "        [20, 20, 20,  ..., 18,  8, 16],\n",
       "        [20, 20, 20,  ..., 18,  8, 16]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_amino_acids_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1001, 116])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_amino_acids_tensor.shape\n",
    "# the shape is 1001 species * 116 amino acids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data_old(input_tensor:torch.Tensor, batch_size:int, mask_token:int):\n",
    "\n",
    "    rows, cols = input_tensor.shape\n",
    "\n",
    "    idx = torch.randint(rows-1, (batch_size,))\n",
    "\n",
    "    input_seqs = []\n",
    "    target_amino_acids = []\n",
    "    mask_positions = []\n",
    "    for i in idx:\n",
    "        # select one amino acid seq\n",
    "        selected_amino_seq = input_tensor[i].clone()\n",
    "        # randomly choose a position to mask\n",
    "        mask_position = torch.randint(cols-1, (1,)) \n",
    "        target_amino_acid = selected_amino_seq[mask_position]\n",
    "        # replace the mask posiiton with mask-token\n",
    "        selected_amino_seq[mask_position] = mask_token\n",
    "        train_input_seq = selected_amino_seq\n",
    "\n",
    "        input_seqs.append(train_input_seq)\n",
    "        target_amino_acids.append(target_amino_acid)\n",
    "        mask_positions.append(mask_position)\n",
    "\n",
    "    return input_seqs, target_amino_acids, mask_positions\n",
    "        \n",
    "\n",
    "# create_training_data(all_amino_acids_tensor, batch_size=64, mask_token=21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(input_tensor: torch.Tensor, batch_size: int, mask_token: int):\n",
    "    \"\"\"\n",
    "    Creates masked training data efficiently using vectorized operations.\n",
    "\n",
    "    Args:\n",
    "      input_tensor (torch.Tensor): Input tensor of shape (num_sequences, sequence_length)\n",
    "      batch_size (int): The desired batch size.\n",
    "      mask_token (int): The token used for masking.\n",
    "\n",
    "    Returns:\n",
    "      tuple: (input_seqs, target_amino_acids, mask_positions)\n",
    "             - input_seqs: Tensor of shape (batch_size, sequence_length) with masked sequences.\n",
    "             - target_amino_acids: Tensor of shape (batch_size,) containing the masked amino acids.\n",
    "             - mask_positions: Tensor of shape (batch_size,) indicating mask positions.\n",
    "    \"\"\"\n",
    "\n",
    "    rows = input_tensor.shape[0]\n",
    "    seq_len = input_tensor.shape[1]\n",
    "    # Randomly select 'batch_size' rows (amino acid sequences)\n",
    "    idx = torch.randint(rows, size=(batch_size,))\n",
    "    input_seqs = input_tensor[idx].clone()\n",
    "\n",
    "    # Generate random mask positions within each selected sequence\n",
    "    mask_positions = torch.randint(seq_len, size=(batch_size, 1))\n",
    "\n",
    "    # Get the target amino acids at the mask positions\n",
    "    target_amino_acids = input_seqs.gather(1, mask_positions).squeeze()\n",
    "\n",
    "    # Create a mask for the selected positions \n",
    "    mask = torch.zeros(input_seqs.size(), dtype=torch.bool)\n",
    "    mask.scatter_(1, mask_positions, 1)\n",
    "\n",
    "    # Apply the mask to replace the target positions with the mask_token\n",
    "    input_seqs[mask] = mask_token\n",
    "\n",
    "    return input_seqs, target_amino_acids, mask_positions.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs, targets, mask_pos = create_training_data(all_amino_acids_tensor, batch_size=32, mask_token=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 116])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exactly one masked value\n",
    "(input_seqs[0] == 21).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 14,  2, 16,  0, 12, 19, 20, 20, 19, 19,  4, 19,  1,  0, 16, 10,  3,\n",
       "         3, 10,  8,  7,  9,  4, 17, 12, 17, 16, 19,  3, 19,  3,  9, 17, 16, 10,\n",
       "        13, 16,  9,  3,  7,  3,  5, 14,  0,  3, 21,  3, 19, 10,  4, 15,  6, 19,\n",
       "         6,  1,  6, 19, 12, 13, 16, 10, 12,  3, 16, 12,  3, 19,  7,  6, 19,  0,\n",
       "         3,  3, 14,  6, 16, 10,  9, 16,  8, 14,  2,  3, 10, 14, 10,  3, 10, 17,\n",
       "         8,  2, 17, 19, 17,  7,  3,  3,  9, 18,  0, 18,  8,  9, 10,  9,  6,  6,\n",
       "         5, 19, 12,  3, 19, 18,  2, 16])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([46, 32, 94, 24, 23, 56, 57, 65, 39, 96, 65, 88, 72, 39, 94, 97, 43, 13,\n",
       "        43,  6, 78, 97, 83, 71, 98, 84, 85, 17, 59,  8,  0, 60])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12,  0, 17, 17,  4,  6, 12, 13, 17,  3, 14, 10,  3, 17, 17,  3, 13,  1,\n",
       "        14, 19,  7,  3, 17,  0, 19, 10,  3,  3, 13, 20,  4, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MaskedAminoSeqDataset(Dataset):\n",
    "    def __init__(self, input_tensor: torch.Tensor, mask_token: int):\n",
    "            \"\"\"\n",
    "            Dataset for masked amino acid sequence prediction.\n",
    "\n",
    "            Args:\n",
    "            input_tensor (torch.Tensor): Input tensor of shape (num_sequences, sequence_length).\n",
    "            mask_token (int): The token used for masking.\n",
    "            \"\"\"\n",
    "            self.input_tensor = input_tensor\n",
    "            self.mask_token = mask_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_tensor.shape[0] \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seqs, target_amino_acids, mask_positions = \\\n",
    "            self._create_training_data(self.input_tensor, batch_size=1, mask_token=self.mask_token)\n",
    "        return input_seqs.squeeze(0), target_amino_acids.squeeze(0), mask_positions.squeeze(0)\n",
    "\n",
    "    def _create_training_data(self, input_tensor: torch.Tensor, batch_size: int, mask_token: int):\n",
    "        \"\"\"\n",
    "        Creates masked training data efficiently using vectorized operations.\n",
    "\n",
    "        Args:\n",
    "        input_tensor (torch.Tensor): Input tensor of shape (num_sequences, sequence_length)\n",
    "        batch_size (int): The desired batch size.\n",
    "        mask_token (int): The token used for masking.\n",
    "\n",
    "        Returns:\n",
    "        tuple: (input_seqs, target_amino_acids, mask_positions)\n",
    "            - input_seqs: Tensor of shape (batch_size, sequence_length) with masked sequences.\n",
    "            - target_amino_acids: Tensor of shape (batch_size,) containing the masked amino acids.\n",
    "            - mask_positions: Tensor of shape (batch_size,) indicating mask positions.\n",
    "        \"\"\"\n",
    "        rows = input_tensor.shape[0]\n",
    "        seq_len = input_tensor.shape[1]\n",
    "        # Randomly select 'batch_size' rows (amino acid sequences)\n",
    "        idx = torch.randint(rows, size=(batch_size,))\n",
    "        input_seqs = input_tensor[idx].clone()\n",
    "\n",
    "        # Generate random mask positions within each selected sequence\n",
    "        mask_positions = torch.randint(seq_len, size=(batch_size, 1))\n",
    "\n",
    "        # Get the target amino acids at the mask positions\n",
    "        target_amino_acids = input_seqs.gather(1, mask_positions).squeeze()\n",
    "\n",
    "        # Create a mask for the selected positions \n",
    "        mask = torch.zeros(input_seqs.size(), dtype=torch.bool)\n",
    "        mask.scatter_(1, mask_positions, 1)\n",
    "\n",
    "        # Apply the mask to replace the target positions with the mask_token\n",
    "        input_seqs[mask] = mask_token\n",
    "\n",
    "        return input_seqs, target_amino_acids, mask_positions.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming input_tensor is your tensor of amino acid sequences\n",
    "masked_amino_seq_dataset = MaskedAminoSeqDataset(all_amino_acids_tensor, mask_token=21) # Assuming 0 is your mask token\n",
    "masked_amino_seq_dataloader = DataLoader(masked_amino_seq_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amino seq with masked: tensor([[ 4, 18,  0,  ..., 15, 12, 16],\n",
      "        [ 4, 18,  0,  ..., 15, 19,  3],\n",
      "        [ 4, 14,  9,  ..., 15, 19, 16],\n",
      "        ...,\n",
      "        [ 4, 14,  9,  ...,  3, 15, 16],\n",
      "        [ 4,  3,  9,  ..., 10, 17, 16],\n",
      "        [ 4,  9,  3,  ..., 21, 17, 16]])\n",
      "target amino acid: tensor([ 8,  5,  0,  3, 19, 11,  6, 10, 16,  3,  9, 10, 18,  2, 10,  9, 20, 17,\n",
      "        18, 18, 14,  1,  9,  8, 14,  3, 19,  8,  6,  6, 16,  3])\n",
      "mask posittion: tensor([102, 108,   4,  76,  30, 111,  69,  88,  64, 113,  71,  16, 101,  93,\n",
      "         88, 114,  49,  39,  36,  75,  81,  13,  48,  20,  43,  85,  66,  90,\n",
      "         56,  69, 100, 113])\n"
     ]
    }
   ],
   "source": [
    "## each iteration now gives a batch with 32 data points.\n",
    "for i in masked_amino_seq_dataloader:\n",
    "    print(f\"amino seq with masked: {i[0]}\")\n",
    "    print(f\"target amino acid: {i[1]}\")\n",
    "    print(f\"mask posittion: {i[2]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding \n",
    "\n",
    "- Amino Acid Embneddings\n",
    "- Position Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'W': 1, 'F': 2, 'S': 3, 'M': 4, 'C': 5, 'Y': 6, 'I': 7, 'D': 8, 'V': 9, 'G': 10, 'H': 11, 'Q': 12, 'K': 13, 'R': 14, 'N': 15, 'P': 16, 'T': 17, 'E': 18, 'L': 19, '-': 20, '[MASK]': 21}\n"
     ]
    }
   ],
   "source": [
    "print(amino_acid_tokenizer.token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amino_acid_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amino_acid_tokenizer.encode('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(22, 100, padding_idx=20)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.nn.Embedding(len(amino_acid_tokenizer), 100, padding_idx=20) # 20 is for '-', we dont want to learn embedding for '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_seq_length=5000):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, embed_size)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 10])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(32, 100, 10)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 10])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = SinusoidalPositionEncoding(embed_size=10, max_seq_length=9)\n",
    "pos(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AminoBERTEmbedding(torch.nn.module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, max_seq_length, dropout=0.1):\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.token = torch.nn.Embedding(vocab_size, embed_size)\n",
    "        self.position = SinusoidalPositionEncoding(embed_size, max_seq_length=max_seq_length)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, seq, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understanding positioning encoding \n",
    "\n",
    "pos_ids = torch.arange(10, dtype=torch.long)\n",
    "pos_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[3,4,7,8,1,3,4,7,4,8], [3,4,7,8,1,3,4,7,4,8]])\n",
    "pos_ids.unsqueeze(0).expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4., 7., 8., 1., 3., 4., 7., 4., 8.],\n",
       "        [3., 4., 7., 8., 1., 3., 4., 7., 4., 8.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinPredictor(nn.Module):\n",
    "    def init(self, num_variants, seq_length, num_amino_acids, emd_dim=128, nhead=8, num_layers=3):\n",
    "        super().init()\n",
    "        \n",
    "        self.embed = nn.Embedding(num_amino_acids, emd_dim)\n",
    "        self.pos_encoder = PositionalEncoding(emd_dim, seq_length)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emd_dim, nhead=nhead)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(emd_dim, num_amino_acids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amino_acid_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Embedding.__init__() missing 1 required positional argument: 'embedding_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m emb_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      5\u001b[0m embed \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size, emb_dim)\n\u001b[0;32m----> 6\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Embedding.__init__() missing 1 required positional argument: 'embedding_dim'"
     ]
    }
   ],
   "source": [
    "vocab_size = len(amino_acid_tokenizer)\n",
    "emb_dim = 8\n",
    "\n",
    "\n",
    "embed = nn.Embedding(vocab_size, emb_dim)\n",
    "pos_emb = nn.Embedding(vocab_size, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 116])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 116, 8])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(input_seqs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, emb_dim, 2) * (-math.log(10000.0) / emb_dim))\n",
    "        pe = torch.zeros(max_len, 1, emb_dim)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, emb_dim)\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, seq_len, emb_dim) with positional encodings added.\n",
    "        \"\"\"\n",
    "        return x + self.pe[:x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoder = PositionalEncoding(emb_dim, max_len=116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (116) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpos_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seqs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deep-learning-genetics/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deep-learning-genetics/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[41], line 18\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m        x: Tensor of shape (batch_size, seq_len, emb_dim)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m        Tensor of shape (batch_size, seq_len, emb_dim) with positional encodings added.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (116) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "pos_encoder(embed(input_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
